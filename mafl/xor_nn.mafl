(##########################################################

773575/1000000 epochs
MFEL 0.6.0 : 55s
MFEL 0.7.0 : 49s
MFEL 0.8.1 : 49s (no gc) 146s (with gc)

###########################################################)

let sigmoid = fun x {
    1.0/(1.0+@exp(-x))
};

let Model = cons w_x1_h1 w_x1_h2 w_x2_h1 w_x2_h2 w_b1_h1 w_b1_h2 w_h1_out w_h2_out w_b2_out {
    let self = @self;

    let forward = fun x1 x2 {
        let h1 = sigmoid(x1 * w_x1_h1 + x2 * w_x2_h1 + w_b1_h1);
        let h2 = sigmoid(x1 * w_x1_h2 + x2 * w_x2_h2 + w_b1_h2);
        let out = sigmoid(h1 * w_h1_out + h2 * w_h2_out + w_b2_out);
        [h1,h2,out]
    };

    let backward = fun out x1 x2 y h1 h2 learning_rate {
        let grad_out = out * (1.0 - out) * (y - out);
        let grad_h1 = h1 * (1.0 - h1) * w_h1_out * grad_out;
        let grad_h2 = h2 * (1.0 - h2) * w_h2_out * grad_out;
        let grad_out_lr = learning_rate * grad_out;
        let grad_h1_lr = learning_rate * grad_h1;
        let grad_h2_lr = learning_rate * grad_h2;
        w_x1_h1 += grad_h1_lr * x1;
        w_x1_h2 += grad_h2_lr * x1;
        w_x2_h1 += grad_h1_lr * x2;
        w_x2_h2 += grad_h2_lr * x2;
        w_b1_h1 += grad_h1_lr;
        w_b1_h2 += grad_h2_lr;
        w_h1_out += grad_out_lr * h1;
        w_h2_out += grad_out_lr * h2;
        w_b2_out += grad_out_lr;
        let diff = y - out;
        let loss = 0.5*diff*diff;
        loss
    };

    let to_string = fun {
        "Model(" + w_x1_h1 + "," + w_x1_h2 + "," + w_x2_h1 + "," + w_x2_h2 + "," +
        w_b1_h1 + "," + w_b1_h2 + "," + w_h1_out + "," + w_h2_out + "," + w_b2_out + ")"
    };

    let train = fun epochs learning_rate target_loss data {
        from @std.iter use range;
        let trace_steps = epochs // 10;
        let start = @now();
        @println(self);
        let loss = for epoch in range(0,epochs) {
            let trace_now = epoch % trace_steps == 0;
            if trace_now { 
                @println("Epoch ",epoch); 
            }
            let max_loss = 0.0;
            for row in data {
                let [x1,x2,y] = row;
                let [h1,h2,out] = forward(x1,x2);
                let loss = backward(out, x1, x2, y, h1, h2, learning_rate);
                if loss > max_loss { 
                    max_loss = loss; 
                }
                if trace_now { 
                    @println("Out(",x1,",",x2,") = ",out,", Expected = ",y,", Loss = ",loss,"");
                }
            }
            if max_loss < target_loss {
                @println("Stopping at epoch ",epoch," with max loss below ",target_loss);
                break max_loss;
            }
            max_loss
        };
        let elapsed = @now() - start;
        @println(self);
        @println("Loss: ",loss);
        @println("Elapsed time: ",elapsed," seconds");
    };
};

let data = [
        [1.0,1.0,0.0],
        [1.0,0.0,1.0],
        [0.0,1.0,1.0],
        [0.0,0.0,0.0]
];

let model = Model(0.5,0.9,0.4,1.0,-0.8,0.1,-1.2,1.1,-0.3);

model.train(1000000, 0.1, 0.00001, data);

